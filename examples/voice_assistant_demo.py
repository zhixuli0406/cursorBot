#!/usr/bin/env python3
"""
CursorBot v1.1 Voice Assistant Demo

This script demonstrates the voice assistant capabilities:
- Voice wake detection
- Speech recognition
- Intent recognition
- Context-aware responses
- Command execution

Prerequisites:
1. Install dependencies:
   pip install vosk edge-tts numpy

2. Download Vosk model:
   Download from: https://alphacephei.com/vosk/models
   Extract to: models/vosk-model-small-cn (for Chinese)
   Or: models/vosk-model-small-en (for English)

3. Run this script:
   python examples/voice_assistant_demo.py

Usage:
- Say "hey cursor" or "å°åŠ©æ‰‹" to wake the assistant
- Then speak your command or question
- The assistant will respond with voice

Example commands:
- "èª¿é«˜éŸ³é‡" - Increase volume
- "æ‰“é–‹ Cursor" - Open Cursor app
- "ç¾åœ¨å¹¾é»" - What time is it
- "æœå°‹å¤©æ°£" - Search weather
- "æé†’æˆ‘10åˆ†é˜å¾Œå–æ°´" - Remind me to drink water in 10 minutes
- "Git æäº¤" - Git commit
"""

import os
import sys
import asyncio

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.core.voice_assistant import (
    VoiceAssistant, VoiceAssistantConfig, AssistantState,
    WakeEngine, STTEngine, TTSEngine,
)
from src.core.voice_llm import (
    VoiceLLM, VoiceLLMConfig, IntegratedVoiceAssistant,
)
from src.core.voice_context import get_context_engine
from src.core.voice_commands import get_command_executor
from src.core.voice_learning import get_learning_engine
from src.utils.logger import logger


async def main():
    """Main demo function."""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           CursorBot v1.1 Voice Assistant Demo                 â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Say "hey cursor" or "å°åŠ©æ‰‹" to wake the assistant          â•‘
â•‘  Then speak your command or question                          â•‘
â•‘  Press Ctrl+C to exit                                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    # Check for required models
    vosk_model_path = "models/vosk-model-small-cn"
    if not os.path.exists(vosk_model_path):
        print(f"""
âš ï¸  Vosk model not found at {vosk_model_path}

Please download a Vosk model:
1. Visit: https://alphacephei.com/vosk/models
2. Download: vosk-model-small-cn (Chinese) or vosk-model-small-en (English)
3. Extract to: models/vosk-model-small-cn

Or set VOICE_VOSK_MODEL_PATH in your .env file.
""")
        # Continue anyway - will fall back to other methods
    
    # Configure the assistant
    config = VoiceAssistantConfig(
        wake_enabled=True,
        wake_engine=WakeEngine.VOSK,
        wake_words=["hey cursor", "ok cursor", "å°åŠ©æ‰‹", "å˜¿ cursor"],
        wake_timeout=10.0,
        stt_engine=STTEngine.WHISPER_LOCAL,
        stt_language="zh",
        tts_engine=TTSEngine.EDGE,
        tts_voice="zh-TW-HsiaoChenNeural",
        vad_enabled=True,
        noise_reduction=True,
        sound_enabled=True,
        vosk_model_path=vosk_model_path,
    )
    
    llm_config = VoiceLLMConfig(
        assistant_name="å°åŠ©æ‰‹",
        language="zh-TW",
        max_response_length=200,
    )
    
    # Create integrated assistant
    assistant = IntegratedVoiceAssistant(config, llm_config)
    
    # Register event handlers
    def on_wake(event):
        print(f"\nğŸ¤ Wake word detected: {event.wake_word}")
        print("   Listening for your command...")
    
    def on_response(response):
        print(f"\nğŸ“ You said: {response.utterance.text}")
        if response.intent:
            print(f"ğŸ¯ Intent: {response.intent.category.value}")
        print(f"ğŸ¤– Response: {response.text}")
        if response.audio:
            print("ğŸ”Š (Playing audio response...)")
    
    assistant.on_wake(on_wake)
    assistant.on_response(on_response)
    
    # Start the assistant
    print("Starting voice assistant...")
    if await assistant.start():
        print("âœ… Voice assistant started successfully!")
        print(f"   State: {assistant.state.value}")
        print(f"   Stats: {assistant.get_stats()}")
        print("\nğŸ§ Listening for wake word...\n")
    else:
        print("âŒ Failed to start voice assistant")
        print("   Check that required dependencies are installed:")
        print("   pip install vosk edge-tts numpy")
        return
    
    # Simulate audio input loop (in real usage, this would be from microphone)
    try:
        # Demo mode - show context and suggestions
        context_engine = get_context_engine()
        context = await context_engine.get_current_context()
        
        print(f"\nğŸ“ Context:")
        print(f"   Time: {context.time.time_of_day.value}")
        print(f"   Device: {context.device.type.value}")
        if context.activity.current_app:
            print(f"   Current app: {context.activity.current_app}")
        
        greeting = context_engine.get_greeting(context)
        print(f"\nğŸ‘‹ {greeting}")
        
        suggestions = context_engine.get_suggestions(context)
        if suggestions:
            print("\nğŸ’¡ Suggestions:")
            for s in suggestions[:3]:
                print(f"   - {s}")
        
        # Show available commands
        executor = get_command_executor()
        print("\nğŸ“‹ Available command categories:")
        for cmd in executor.get_available_commands():
            print(f"   - {cmd}")
        
        # Demo: Process sample commands (simulate)
        print("\n" + "="*60)
        print("Demo mode: Processing sample commands...")
        print("="*60)
        
        sample_commands = [
            "ç¾åœ¨å¹¾é»",
            "æ‰“é–‹ Cursor",
            "Git status",
            "æé†’æˆ‘ä¸‹åˆé–‹æœƒ",
        ]
        
        from src.core.voice_assistant import Utterance, IntentRecognizer
        recognizer = IntentRecognizer(config)
        
        for cmd in sample_commands:
            print(f"\nğŸ“¢ Command: {cmd}")
            
            # Recognize intent
            intent = await recognizer.recognize(cmd)
            print(f"   Intent: {intent.category.value}")
            
            # Execute command
            result = await executor.execute(intent)
            print(f"   Result: {result.status.value}")
            if result.response_text:
                print(f"   Response: {result.response_text}")
        
        print("\n" + "="*60)
        print("Demo complete!")
        print("="*60)
        
        # Keep running until interrupted
        print("\nğŸ§ Voice assistant is ready. Press Ctrl+C to exit.\n")
        
        # In production, this would be an audio capture loop
        # For demo, we just wait
        while True:
            await asyncio.sleep(1)
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Shutting down voice assistant...")
    finally:
        await assistant.stop()
        print("âœ… Voice assistant stopped.")


# Text-based demo (no microphone required)
async def text_demo():
    """Text-based demo for testing without microphone."""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘       CursorBot v1.1 Voice Assistant - Text Demo              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Type commands as if speaking to test the assistant           â•‘
â•‘  Type 'quit' to exit                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    from src.core.voice_assistant import Utterance, IntentRecognizer, VoiceAssistantConfig
    from src.core.voice_commands import get_command_executor
    from src.core.voice_context import get_context_engine
    from src.core.voice_learning import get_learning_engine
    
    config = VoiceAssistantConfig()
    recognizer = IntentRecognizer(config)
    executor = get_command_executor()
    context_engine = get_context_engine()
    learning_engine = get_learning_engine()
    
    # Get initial context
    context = await context_engine.get_current_context()
    print(f"\nğŸ‘‹ {context_engine.get_greeting(context)}")
    
    suggestions = context_engine.get_personalized_suggestions()
    if suggestions:
        print("\nğŸ’¡ Based on your habits, you might want to:")
        for s in suggestions[:3]:
            print(f"   - {s}")
    
    print("\nğŸ“‹ Available commands:")
    for cmd in executor.get_available_commands():
        print(f"   - {cmd}")
    
    print("\n" + "-"*60)
    
    while True:
        try:
            text = input("\nğŸ¤ You: ").strip()
            if not text:
                continue
            if text.lower() in ['quit', 'exit', 'é€€å‡º']:
                break
            
            # Create utterance
            utterance = Utterance(text=text)
            
            # Recognize intent
            intent = await recognizer.recognize(text)
            print(f"   ğŸ¯ Intent: {intent.category.value}")
            
            # Execute command
            result = await executor.execute(intent)
            
            # Record interaction for learning
            await learning_engine.record_interaction(
                utterance=utterance,
                intent=intent,
                response=result.response_text,
                command_executed=result.status.value == "success",
                success=result.status.value != "failed"
            )
            
            if result.response_text:
                print(f"   ğŸ¤– {result.response_text}")
            else:
                # If no specific response, generate one
                print(f"   ğŸ¤– å¥½çš„ï¼Œæˆ‘ç†è§£ä½ èªªçš„æ˜¯ã€Œ{text}ã€")
            
        except KeyboardInterrupt:
            print("\n")
            break
        except Exception as e:
            print(f"   âŒ Error: {e}")
    
    print("\nğŸ“Š Session statistics:")
    stats = learning_engine.get_statistics()
    print(f"   Total interactions: {stats['total_interactions']}")
    print(f"   Shortcuts: {stats['shortcuts_count']}")
    print(f"   Patterns learned: {stats['patterns_learned']}")
    
    print("\nğŸ‘‹ Goodbye!")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="CursorBot Voice Assistant Demo")
    parser.add_argument(
        "--text", "-t",
        action="store_true",
        help="Run text-based demo (no microphone required)"
    )
    args = parser.parse_args()
    
    if args.text:
        asyncio.run(text_demo())
    else:
        asyncio.run(main())
